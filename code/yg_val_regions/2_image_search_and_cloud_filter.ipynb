{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf9fe9a",
   "metadata": {},
   "source": [
    "# Planet Image Search and MODIS Cloud Cover Filter\n",
    "### This script searches Planet PSScene images and filters the search results by cloud cover using MODIS and Planet derived cloud cover metrics.\n",
    "#### TODO:\n",
    "- figure out how to ensure coverage in all cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3197feca",
   "metadata": {},
   "source": [
    "## API and Package Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb68e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Earth Engine API\n",
    "import ee\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d783ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import geemap\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapely as shp\n",
    "from pprint import pprint\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f003ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Planet API Key\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "api_key = os.getenv('PL_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dda253",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c91db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import MODIS Data\n",
    "modis = ee.ImageCollection('MODIS/061/MOD09GA');\n",
    "\n",
    "# MODIS snow\n",
    "modis_snow = ee.ImageCollection('MODIS/006/MOD10A1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad44ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shapefile with AOI (multipolygon)\n",
    "aoi = gpd.read_file(\"/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/yg_val_regions/bboxes/yg_validation_bboxes.shp\")\n",
    "# convert from multipolygon to multiple polygons\n",
    "aoi = aoi.explode(column = 'geometry', ignore_index = True)\n",
    "# remove inner holes\n",
    "aoi.geometry = aoi.geometry.exterior\n",
    "# convert back to polygon\n",
    "aoi.geometry = [shp.geometry.Polygon([shp.geometry.Point(x, y) for x, y in list(feature.coords)]) for feature in aoi.geometry]\n",
    "aoi['region'] = aoi.index\n",
    "# convert to json for planet data search\n",
    "sites = json.loads(aoi.to_json()) # if multiple sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b7eefd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae936300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Years to test\n",
    "years = [2017, 2018, 2019, 2020, 2021]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1df64f1",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aef3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to extract specific bits from bitmask\n",
    "def bitwiseExtract(input, fromBit, toBit):\n",
    "    maskSize = ee.Number(1).add(toBit).subtract(fromBit)\n",
    "    mask = ee.Number(1).leftShift(maskSize).subtract(1)\n",
    "    return input.rightShift(fromBit).bitwiseAnd(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c904d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to get snow data from MODIS\n",
    "def modisSnow(modis_snow_imagery, date, aoi):\n",
    "    # extract NDSI\n",
    "    snow_cover = (ee.Image(modis_snow_imagery\n",
    "                           .filter(ee.Filter.date(date))\n",
    "                           .select(['NDSI_Snow_Cover'])\n",
    "                           .first())\n",
    "                  .clip(aoi));\n",
    "    \n",
    "    # get average snow cover\n",
    "    snow_cover = snow_cover.reduceRegion(ee.Reducer.max(), aoi);\n",
    "    \n",
    "    return snow_cover.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f2fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to figure out UTM zone from lon\n",
    "def long2UTM(long):\n",
    "    zone = int(np.floor((long + 180)/6) % 60) + 1\n",
    "    return 'EPSG:'+ str(32600 + zone) # would be 32700 for Southern Hemisphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18654c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to extract metadata needed for cloud calculation\n",
    "def getMetadata(feature, aoi, region, crop_aoi = None):\n",
    "    \n",
    "    # crop_aoi = aoi, if no crop_aoi provided\n",
    "    if crop_aoi is None:\n",
    "        crop_aoi = aoi\n",
    "    \n",
    "    # get image id\n",
    "    img_id = feature['id']\n",
    "    \n",
    "    # get image date\n",
    "    img_date = feature['properties']['acquired'].split('T')[0]\n",
    "    \n",
    "    # get instrument type\n",
    "    instrument_type = feature['properties']['instrument']\n",
    "    \n",
    "    # get planet cloud/cloud shadow cover\n",
    "    \n",
    "    try:\n",
    "        img_cloud_cover = float(100 - feature['properties']['clear_percent'])\n",
    "    except KeyError:\n",
    "        img_cloud_cover = float(feature['properties']['cloud_cover']*100)\n",
    "    \n",
    "    # use intersection of aoi and search result geometry to get actual geometry of cells with data\n",
    "    img_geometry = (\n",
    "        aoi[aoi.region == region].geometry\n",
    "        .intersection(\n",
    "            shp.geometry.Polygon(\n",
    "                tuple([(feature[0], feature[1]) for feature in feature['geometry']['coordinates'][0]])\n",
    "            )\n",
    "        )\n",
    "        .reset_index(drop = True)\n",
    "    )\n",
    "    img_geometry_crop = (\n",
    "        crop_aoi[crop_aoi.region == region].geometry\n",
    "        .intersection(\n",
    "            shp.geometry.Polygon(\n",
    "                tuple([(feature[0], feature[1]) for feature in feature['geometry']['coordinates'][0]])\n",
    "            )\n",
    "        )\n",
    "        .reset_index(drop = True)\n",
    "    )\n",
    "    \n",
    "    # get image area and coverage\n",
    "    if not img_geometry[0].is_empty:\n",
    "        # Determine crs\n",
    "        if str(type(img_geometry[0])) == \"<class 'shapely.geometry.polygon.Polygon'>\":\n",
    "            crs = long2UTM(list(img_geometry.exterior[0].coords)[0][0])\n",
    "        elif str(type(img_geometry[0])) == \"<class 'shapely.geometry.multipolygon.MultiPolygon'>\":\n",
    "            crs = long2UTM(list(img_geometry[0].geoms)[0].exterior.coords[0][0])\n",
    "        elif str(type(img_geometry[0])) == \"<class 'shapely.geometry.collection.GeometryCollection'>\":\n",
    "            polygon_idx = [item for item in list(img_geometry[0].geoms) if str(type(item)) == \"<class 'shapely.geometry.polygon.Polygon'>\"]\n",
    "            crs = long2UTM(list(img_geometry[0].geoms)[0].exterior.coords[0][0])\n",
    "    # calculate area and % coverage in correct crs\n",
    "    if img_geometry[0].is_empty or str(type(img_geometry[0])) == \"<class 'shapely.geometry.linestring.LineString'>\":\n",
    "        crs = None\n",
    "        img_area = 0.0\n",
    "        img_coverage = 0\n",
    "    else:\n",
    "        img_area = float(img_geometry_crop.to_crs(crs = crs).area/1e6)\n",
    "        img_coverage = round(float(img_geometry.to_crs(crs = crs).area)/\n",
    "                             float(aoi[aoi.region == region].geometry.to_crs(crs = crs).area)*100)\n",
    "    \n",
    "    return [img_id, img_date, instrument_type, img_coverage, img_area, img_cloud_cover, img_geometry, crs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df3fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometryToEE(img_geometry, region):\n",
    "    \n",
    "    # format geometry\n",
    "    img_geometry = gpd.GeoDataFrame(geometry = img_geometry)\n",
    "    img_geometry = [[[x, y] for x, y in list(img_geometry.geometry.exterior[0].coords)]]\n",
    "    \n",
    "    # convert geometry to ee.Geometry\n",
    "    img_geometry_ee = ee.Geometry({\n",
    "        'type': 'Polygon',\n",
    "        'coordinates': img_geometry\n",
    "    })\n",
    "    \n",
    "    return img_geometry_ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to calculate MODIS cloud cover\n",
    "def modisCloudCover(modis_imagery, date, aoi):\n",
    "    # extract QC bitmask band from MODIS\n",
    "    qc = (ee.Image(modis_imagery\n",
    "              .filter(ee.Filter.date(date))\n",
    "              .first())\n",
    "      .select(['state_1km'])\n",
    "      .clip(aoi));\n",
    "    \n",
    "    # extract cloud information from MODIS QC bitmask\n",
    "    cloud_mask = bitwiseExtract(qc, 0, 1).remap([0, 1, 2, 3], [0, 1, 1, 1])\n",
    "    area_mask = cloud_mask.remap([0, 1], [1, 1])\n",
    "    \n",
    "    # calculate area of cells with clouds\n",
    "    cloud_area_img = cloud_mask.multiply(ee.Image.pixelArea())\n",
    "    area_img = area_mask.multiply(ee.Image.pixelArea())\n",
    "    \n",
    "    # calculate cloud cover percent\n",
    "    cloud_area = (\n",
    "        cloud_area_img\n",
    "        .reduceRegion(\n",
    "            reducer = ee.Reducer.sum(), # calculate total cloud area\n",
    "            geometry = aoi,\n",
    "            scale = 1000,\n",
    "            maxPixels = 1e10\n",
    "        )\n",
    "        .getNumber('remapped')\n",
    "        .divide(\n",
    "            area_img\n",
    "            .reduceRegion(\n",
    "                reducer = ee.Reducer.sum(), # calculate total area\n",
    "                geometry = aoi,\n",
    "                scale = 1000,\n",
    "                maxPixels = 1e10\n",
    "            )\n",
    "            .getNumber('remapped')\n",
    "        ) # divide cloud area by total area\n",
    "        .multiply(ee.Number(100)) # convert to %\n",
    "        .round() # remove decimal precision\n",
    "    );\n",
    "    \n",
    "    return cloud_area.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b9edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download with retry\n",
    "def recu_down(url, filename): # recurrent download with ContentTooShortError\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url,filename)\n",
    "    except urllib.error.ContentTooShortError:\n",
    "        print('Download failed. Trying again...')\n",
    "        recu_down(url, filename)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2abe2a9",
   "metadata": {},
   "source": [
    "## Determine Snow Free Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e1e47f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# modis_snow_df = pd.DataFrame(columns = ['region', 'year', 'date', 'snow_cover'])\n",
    "# for index, row in aoi.iterrows():\n",
    "#     print(index)\n",
    "#     geometry = [[[x, y] for x, y in list(row.geometry.exterior.coords)]]\n",
    "    \n",
    "#     # convert geometry to ee.Geometry\n",
    "#     geometry_ee = ee.Geometry({\n",
    "#         'type': 'Polygon',\n",
    "#         'coordinates': geometry\n",
    "#     })\n",
    "#     for year in years:\n",
    "#         print(year)\n",
    "#         dates = pd.date_range(str(year) + '-06-01', str(year) + '-08-31', freq = 'D')\n",
    "        \n",
    "#         for date in dates:\n",
    "            \n",
    "#             snow = modisSnow(modis_snow, date, geometry_ee)\n",
    "            \n",
    "#             # add to output\n",
    "#             modis_snow_df = pd.concat([modis_snow_df,\n",
    "#                                        pd.DataFrame({'region': index,\n",
    "#                                                      'year': year,\n",
    "#                                                      'date': date,\n",
    "#                                                      'snow_cover': snow})])\n",
    "\n",
    "# modis_snow_df = modis_snow_df.fillna(value = np.NAN)\n",
    "# modis_snow_df = modis_snow_df.reset_index(drop = True)\n",
    "# modis_snow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a6d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modis_snow_df.to_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/yg_val_regions/modis_snow_data.csv',\n",
    "#                     index = False)\n",
    "modis_snow_df = pd.read_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/yg_val_regions/modis_snow_data.csv')\n",
    "modis_snow_df['date'] = pd.to_datetime(modis_snow_df['date'])\n",
    "modis_snow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7673480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define first snow-free date as first of three consecutive NDSI == 0 (removing NaN values)\n",
    "modis_snow_df['snow_free'] = np.where(modis_snow_df.snow_cover == 0, 1, 0)\n",
    "modis_snow_df['rolling_snow_free'] = (\n",
    "    modis_snow_df\n",
    "    .groupby(['region', 'year'])\n",
    "    .snow_free\n",
    "    .rolling(2).sum().shift(-1)\n",
    "    .reset_index(drop = True)\n",
    ")\n",
    "snow_free_date = (\n",
    "    modis_snow_df[modis_snow_df.rolling_snow_free == 2]\n",
    "    .groupby(['region', 'year'])\n",
    "    .first()\n",
    "    .rename(columns = {'date': 'snow_free_date'})\n",
    "    .snow_free_date\n",
    ")\n",
    "\n",
    "snow_free_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08119a3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "with warnings.catch_warnings(): # there is a warning getting triggered inside of sns, I think\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    g = sns.FacetGrid(data = modis_snow_df,\n",
    "                          col = 'year',\n",
    "                          row = 'region',\n",
    "                          sharex = False)\n",
    "    g.map(sns.lineplot, 'date', 'snow_cover')\n",
    "    \n",
    "    for ax, pos in zip(g.axes.flat, snow_free_date):\n",
    "        ax.axvline(x=pos, color='black', linestyle=':')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f8c15",
   "metadata": {},
   "source": [
    "## Search Planet Imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf265e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_metadata = []\n",
    "# metadata_df = pd.DataFrame(columns = ['region', 'year', 'page', 'metadata'])\n",
    "\n",
    "# # Data type\n",
    "# item_type = \"PSScene\"\n",
    "\n",
    "# # asset filter\n",
    "# asset_filter = {\n",
    "#     'type': 'OrFilter',\n",
    "#     'config': [\n",
    "#        {\n",
    "#            \"type\": \"AndFilter\",\n",
    "#             \"config\": [\n",
    "#                 {\n",
    "#                     \"type\": \"AssetFilter\",\n",
    "#                     \"config\": [\n",
    "#                         \"ortho_analytic_4b_sr\"\n",
    "#                     ]\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"type\": \"AssetFilter\",\n",
    "#                     \"config\": [\n",
    "#                         \"ortho_udm2\"\n",
    "#                     ]\n",
    "#                 }\n",
    "#             ]\n",
    "#         },\n",
    "#         {\n",
    "#             \"type\": \"AndFilter\",\n",
    "#             \"config\": [\n",
    "#                 {\n",
    "#                     \"type\": \"AssetFilter\",\n",
    "#                     \"config\": [\n",
    "#                         \"ortho_analytic_8b_sr\"\n",
    "#                     ]\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"type\": \"AssetFilter\",\n",
    "#                     \"config\": [\n",
    "#                         \"ortho_udm2\"\n",
    "#                     ]\n",
    "#                 }\n",
    "#             ]\n",
    "#         } \n",
    "#     ]\n",
    "    \n",
    "# }\n",
    "\n",
    "# for region, site in enumerate(sites['features']):\n",
    "    \n",
    "#     print('Region: ' + site['id'])\n",
    "#     site_name = site['id']\n",
    "\n",
    "#     session = requests.Session()\n",
    "#     session.auth = (api_key, '')\n",
    "#     site_coords = site['geometry']['coordinates']\n",
    "\n",
    "#     site_dict = {\n",
    "#         \"type\": \"Polygon\",\n",
    "#         \"coordinates\": site_coords}\n",
    "\n",
    "#     # get images that overlap with our aoi\n",
    "#     geometry_filter = {\n",
    "#         \"type\": \"GeometryFilter\",\n",
    "#         \"field_name\": \"geometry\",\n",
    "#         \"config\": site_dict\n",
    "#     }\n",
    "\n",
    "#     for year in years:\n",
    "        \n",
    "#         if snow_free_date.loc[(region, year)] < pd.to_datetime('{}-07-01'.format(year)):\n",
    "#             start_date = str(snow_free_date.loc[(region, year)])[0:10]\n",
    "#         else:\n",
    "#             start_date = '{}-07-01'.format(year)\n",
    "\n",
    "#         # i only want images between these two dates of each year...easier to search within a year to avoid massive search queries\n",
    "#         start_date = \"{}T00:00:00.000Z\".format(start_date)\n",
    "#         end_date = \"{}-09-30T00:00:00.000Z\".format(year)\n",
    "\n",
    "#         # get images acquired within a date range\n",
    "#         date_range_filter = {\n",
    "#             \"type\": \"DateRangeFilter\",\n",
    "#             \"field_name\": \"acquired\",\n",
    "#             \"config\": {\n",
    "#                 \"gte\": start_date,\n",
    "#                 \"lte\": end_date\n",
    "#             }\n",
    "#         }\n",
    "\n",
    "#         cloud_cover_filter = {\n",
    "#             \"type\": \"RangeFilter\",\n",
    "#             \"field_name\": \"cloud_cover\",\n",
    "#             \"config\": {\n",
    "#                 \"lte\": 1 # cloud cover threshold - none currently\n",
    "#             }\n",
    "#         }\n",
    "\n",
    "#         combined_filter = {\n",
    "#             \"type\": \"AndFilter\",\n",
    "#             \"config\": [\n",
    "#                 asset_filter,\n",
    "# #                 instrument_filter,\n",
    "#                 geometry_filter,\n",
    "#                 date_range_filter,\n",
    "#                 cloud_cover_filter\n",
    "#             ]\n",
    "#         }\n",
    "\n",
    "#         # API request object\n",
    "#         search_request = {\n",
    "#             \"item_types\": [item_type],\n",
    "#             \"filter\": combined_filter\n",
    "#         }\n",
    "\n",
    "#         # fire off the POST request\n",
    "#         search_result = \\\n",
    "#           requests.post(\n",
    "#             'https://api.planet.com/data/v1/quick-search',\n",
    "#             auth=HTTPBasicAuth(api_key, ''),\n",
    "#             json=search_request)\n",
    "\n",
    "#         all_metadata.append(search_result.json())\n",
    "        \n",
    "#         page = 1\n",
    "#         print('Page: ' + str(page))\n",
    "        \n",
    "#         # format metadata for dataframe\n",
    "#         temp_df = pd.DataFrame({'region': site_name,\n",
    "#                                 'year': year,\n",
    "#                                 'page': page,\n",
    "#                                 'metadata': [search_result.json()]})\n",
    "        \n",
    "#         metadata_df = pd.concat([metadata_df, temp_df], axis = 0)\n",
    "#         print('Results from page ' + str(page) + ' added.')\n",
    "\n",
    "#         try:\n",
    "#             next_link = search_result.json()['_links']['_next']\n",
    "            \n",
    "#             while next_link:\n",
    "#                 page = page + 1\n",
    "#                 print('Page: ' + str(page))\n",
    "#                 search_result = \\\n",
    "#                   requests.get(\n",
    "#                     next_link,\n",
    "#                     auth=HTTPBasicAuth(api_key, ''))\n",
    "                \n",
    "#                 all_metadata.append(search_result.json())\n",
    "\n",
    "#                 # format metadata for dataframe\n",
    "#                 temp_df = pd.DataFrame({'region': site_name,\n",
    "#                                         'year': year,\n",
    "#                                         'page': page,\n",
    "#                                         'metadata': [search_result.json()]})\n",
    "#                 metadata_df = pd.concat([metadata_df, temp_df], axis = 0)\n",
    "#                 print('Results from page ' + str(page) + ' added.')\n",
    "\n",
    "#                 try:\n",
    "#                     next_link = search_result.json()['_links']['_next']\n",
    "#                 except:\n",
    "#                     next_link = None\n",
    "#         except:\n",
    "#             next_link = None\n",
    "            \n",
    "#     print('\\n')\n",
    "            \n",
    "# metadata_df = metadata_df.reset_index(drop = True)\n",
    "# metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e82cfd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # save output\n",
    "# metadata_df.to_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/yg_val_regions/planet_image_search.csv')\n",
    "metadata_df = pd.read_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/yg_val_regions/planet_image_search.csv',\n",
    "                          index_col=0,\n",
    "                          converters = {'metadata': ast.literal_eval})\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb127ba",
   "metadata": {},
   "source": [
    "## Calculate Cloud Cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e1eda5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cloud_data = pd.DataFrame(\n",
    "#     columns = [\n",
    "#         'region', \n",
    "#         'year', \n",
    "#         'date', \n",
    "#         'id',\n",
    "#         'instrument',\n",
    "#         'coverage', \n",
    "#         'area', \n",
    "#         'modis_cloud_cover', \n",
    "#         'planet_cloud_cover', \n",
    "#         'cloud_cover', \n",
    "#         'modis_planet_diff',\n",
    "#         'geometry']\n",
    "# )\n",
    "# for index,row in metadata_df.iterrows():\n",
    "    \n",
    "#     print(index);\n",
    "    \n",
    "#     # get metadata\n",
    "#     region = int(row['region']);\n",
    "#     img_year = int(row['year']);\n",
    "#     metadata = row['metadata']['features'];\n",
    "    \n",
    "#     if len(metadata) > 0: \n",
    "#         for feature in metadata:\n",
    "            \n",
    "#             # extract metadata needed for cloud cover calculations\n",
    "#             img_id, img_date, instrument_type, img_coverage, img_area, img_cloud_cover, img_geometry, crs = getMetadata(feature, aoi, region);\n",
    "# #             print(img_id)\n",
    "            \n",
    "#             # calculate modis cloud cover if the geometry is a polygon\n",
    "#             if str(type(img_geometry[0])) == \"<class 'shapely.geometry.polygon.Polygon'>\":\n",
    "#                 # convert geometry to ee.Geometry\n",
    "#                 img_geometry_ee = geometryToEE(img_geometry, region);\n",
    "\n",
    "#                 # calc modis cloud cover\n",
    "#                 modis_cloud_cover = modisCloudCover(modis, img_date, img_geometry_ee);\n",
    "#                 if modis_cloud_cover > 100:\n",
    "#                     modis_cloud_cover = 100;\n",
    "                \n",
    "#             # calculate modis cloud cover if the geometry is a multipolygon\n",
    "#             elif str(type(img_geometry[0])) == \"<class 'shapely.geometry.multipolygon.MultiPolygon'>\":\n",
    "                \n",
    "#                 polygon_geometries = gpd.GeoDataFrame(\n",
    "#                     geometry = img_geometry\n",
    "#                 ).explode(column = 'geometry', ignore_index = True)\n",
    "#                 modis_cloud_cover = []\n",
    "#                 polygon_area = []\n",
    "#                 for index, row in polygon_geometries.iterrows():\n",
    "                    \n",
    "#                     # calculate area of sub polygon\n",
    "#                     temp_area = gpd.GeoDataFrame(\n",
    "#                         geometry = row, \n",
    "#                         crs = polygon_geometries.crs\n",
    "#                     ).reset_index().to_crs(crs = crs).geometry.area/1e6\n",
    "#                     polygon_area.append(temp_area)\n",
    "                    \n",
    "#                     # convert geometry to ee.Geometry\n",
    "#                     polygon_geometry = [[[x, y] for x, y in list(row.geometry.exterior.coords)]]\n",
    "    \n",
    "#                     # convert geometry to ee.Geometry\n",
    "#                     polygon_geometry_ee = ee.Geometry({\n",
    "#                         'type': 'Polygon',\n",
    "#                         'coordinates': polygon_geometry\n",
    "#                     })\n",
    "\n",
    "#                     # calc modis cloud cover of sub polygon\n",
    "#                     temp_cloud_cover = modisCloudCover(modis, img_date, polygon_geometry_ee);\n",
    "#                     modis_cloud_cover.append(temp_cloud_cover)\n",
    "                    \n",
    "#                 # calculate average cloud cover of multipolygon (img_geometry)\n",
    "#                 modis_cloud_cover = int(round(sum(\n",
    "#                     [cloud * area for cloud, area in zip(modis_cloud_cover, polygon_area)]\n",
    "#                 )/img_area))\n",
    "#                 if modis_cloud_cover > 100:\n",
    "#                     modis_cloud_cover = 100;\n",
    "                    \n",
    "#             # get higher cloud cover estimate\n",
    "#             cloud_cover = np.maximum(modis_cloud_cover, img_cloud_cover)\n",
    "            \n",
    "#             # calculate difference between cloud cover estimates\n",
    "#             modis_planet_diff = abs(\n",
    "#                 modis_cloud_cover - img_cloud_cover\n",
    "#             )\n",
    "            \n",
    "#             # organize data into a dataframe\n",
    "#             temp_df = pd.DataFrame({\n",
    "#                 'region': region,\n",
    "#                 'year': img_year,\n",
    "#                 'date': img_date,\n",
    "#                 'id': img_id,\n",
    "#                 'instrument': instrument_type,\n",
    "#                 'coverage': img_coverage,\n",
    "#                 'area': img_area,\n",
    "#                 'modis_cloud_cover': modis_cloud_cover,\n",
    "#                 'planet_cloud_cover': img_cloud_cover,\n",
    "#                 'cloud_cover': cloud_cover,\n",
    "#                 'modis_planet_diff': modis_planet_diff,\n",
    "#                 'geometry': img_geometry\n",
    "#             });\n",
    "\n",
    "#             # append new data to old\n",
    "#             cloud_data = pd.concat([cloud_data, temp_df], axis = 0);\n",
    "\n",
    "# cloud_data = gpd.GeoDataFrame(cloud_data);\n",
    "# cloud_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9996972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # save output\n",
    "# cloud_data.to_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/yg_val_regions/planet_images_modis_cloud.csv')\n",
    "cloud_data = pd.read_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/yg_val_regions/planet_images_modis_cloud.csv',\n",
    "                  index_col = 0)\n",
    "cloud_data = gpd.GeoDataFrame(cloud_data,\n",
    "                              geometry = gpd.GeoSeries.from_wkt(cloud_data['geometry']),\n",
    "                              crs = 'EPSG:4326')\n",
    "cloud_data['date'] = pd.to_datetime(cloud_data['date'])\n",
    "cloud_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f06c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data = cloud_data, \n",
    "            x = 'area',\n",
    "#             hue = 'year',\n",
    "#             multiple = 'stack',\n",
    "#             alpha = 0.5,\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc151bb",
   "metadata": {},
   "source": [
    "## Filter Images on Cloud Cover and Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ce74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get count of images by polygon and year\n",
    "image_counts = cloud_data[['region', 'year']].value_counts()\n",
    "image_counts = pd.DataFrame(image_counts, columns = ['count']).sort_index()\n",
    "# image_counts.to_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/yg_val_regions/image_counts_pre_filter.csv')\n",
    "image_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024972e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter by area >= 10 km^2 to ensure at least a few tie points for AROSICS\n",
    "# Don't filter on cloud cover yet?\n",
    "potential_images = cloud_data[cloud_data['area'] >= 10]\n",
    "potential_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddfb501",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertain_imgs = pd.DataFrame(potential_images[(potential_images['modis_planet_diff'] > 80) \n",
    "                                               & (potential_images['planet_cloud_cover'] < 20)].id)\n",
    "uncertain_imgs['keep'] = None\n",
    "uncertain_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae94df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of thumbnail urls\n",
    "thumbnails = [\n",
    "    feature['_links']['thumbnail'] \n",
    "    for search in metadata_df.metadata \n",
    "    for feature in search['features'] \n",
    "    if feature['id'] in [\n",
    "        str(image) \n",
    "        for image in potential_images[(potential_images['modis_planet_diff'] > 80) \n",
    "                                      & (potential_images['planet_cloud_cover'] < 20)].id\n",
    " ]\n",
    "]\n",
    "print(len(thumbnails))\n",
    "def remove_duplicates(sequence):\n",
    "    lst = []\n",
    "    for i in sequence:\n",
    "        if i not in lst:\n",
    "            lst.append(i)\n",
    "    # returns unsorted unique list\n",
    "    return lst\n",
    "\n",
    "thumbnails = remove_duplicates(thumbnails)\n",
    "print(len(thumbnails))\n",
    "thumbnails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc33114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # preview thumbnails to inspect\n",
    "# for link in thumbnails:\n",
    "#     img_id = link.split('/')[8]\n",
    "#     print('Viewing thumbnail for ' + str(img_id))\n",
    "#     response = requests.get(link,\n",
    "#                     auth=HTTPBasicAuth(api_key, ''))\n",
    "#     thumbnail = Image.open(BytesIO(response.content))\n",
    "#     plt.imshow(thumbnail)\n",
    "#     plt.show()\n",
    "    \n",
    "#     # provide input about whether to download the image\n",
    "#     keep = input('Do you want to download this image?(y/n)\\n')\n",
    "    \n",
    "#     # save user input with img_id to use as a filter later\n",
    "#     uncertain_imgs.loc[uncertain_imgs.id == str(img_id), 'keep'] = keep\n",
    "#     print(uncertain_imgs.loc[uncertain_imgs.id == str(img_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fc0677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# uncertain_imgs.to_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/yg_val_regions/uncertain_images.csv',\n",
    "#                       index = False)\n",
    "uncertain_imgs = pd.read_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/yg_val_regions/uncertain_images.csv')\n",
    "uncertain_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c49b55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "potential_images = pd.merge(potential_images,\n",
    "                            uncertain_imgs,\n",
    "                            how = \"outer\",\n",
    "                            on = 'id')\n",
    "potential_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdafdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use planet_cloud_cover number in cases where high MODIS cloud cover has been determined to be incorrect\n",
    "potential_images.loc[(potential_images.modis_planet_diff > 80) \n",
    "                     & (potential_images.planet_cloud_cover < 20)\n",
    "                     & (potential_images.keep == 'y'),\n",
    "    'cloud_cover'\n",
    "] = potential_images.planet_cloud_cover\n",
    "potential_images.loc[potential_images.keep == 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5b5e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create list of dates in order of preference which will be used to sort the dataframe so that we can slice it to get image dates\n",
    "date_order = pd.DataFrame()\n",
    "for polygon_id in aoi.index:\n",
    "    for year in potential_images['year'].unique():\n",
    "        if snow_free_date[polygon_id, year] <= pd.Timestamp(str(year) + '-07-01'):\n",
    "            start_date = snow_free_date[polygon_id, year]\n",
    "        else:\n",
    "            start_date = pd.Timestamp(str(year) + '-07-01')\n",
    "        dates = pd.DataFrame({'date': pd.date_range(start = start_date,\n",
    "                                                    end = str(year) + '-08-31',\n",
    "                                                    freq = 'D').strftime('%Y-%m-%d')})\n",
    "        split_loc = np.where([bool(re.search('08-01', date)) for date in dates.date])[0][0]\n",
    "        dates_1 = np.flip(np.arange(0, split_loc))\n",
    "        dates_2 = np.arange(split_loc, len(dates))\n",
    "        idx = list(np.insert(dates_1, np.arange(0, len(dates_2)), dates_2))\n",
    "        dates = dates.iloc[idx].reset_index(drop = True).rename(columns = {0: 'date'})\n",
    "        dates['polygon_id'] = polygon_id\n",
    "        dates['year'] = year\n",
    "        dates['idx'] = dates.index.astype('Int32')\n",
    "        dates['date'] = pd.to_datetime(dates['date'])\n",
    "        dates = dates.set_index(['polygon_id', 'year', 'date'])\n",
    "        date_order = pd.concat([date_order, dates])\n",
    "date_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392568dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter images based on clouds and dates\n",
    "images_ordered = pd.DataFrame(columns = potential_images.columns)\n",
    "for region in aoi.index:\n",
    "    print('###################################################')\n",
    "    print(region)\n",
    "    print('###################################################')\n",
    "    \n",
    "    polygon_geometry = (\n",
    "        aoi[region:region+1].reset_index()\n",
    "        .rename(columns = {'index': 'region'})\n",
    "        .loc[:, ['region','geometry']]\n",
    "    )\n",
    "    \n",
    "    for year in potential_images['year'].unique():\n",
    "        print(year)\n",
    "        \n",
    "        # get data\n",
    "        temp_data = potential_images[(potential_images['region'] == region) &\n",
    "                                     (potential_images['year'] == year)]\n",
    "        \n",
    "        # first get all images with no clouds\n",
    "        temp_output = (\n",
    "            temp_data[(temp_data['cloud_cover'] == 0)]\n",
    "            # arrange in correct date order\n",
    "            .join(date_order, on = ['region', 'year', 'date'])\n",
    "            .sort_values(by = ['idx'])\n",
    "            .reset_index(drop = True)\n",
    "        )\n",
    "        \n",
    "        ### It would be nice to get an image count across the entire image and make sure each location\n",
    "        ### has at least 10, but I haven't been able to figure out how\n",
    "#         # check number of images\n",
    "#         polygon_geometry['n_images'] = 0\n",
    "#         n_images = temp_output.loc[:, ['region', 'geometry']]\n",
    "#         n_images['n_images'] = 1\n",
    "#         n_images = pd.concat([n_images, polygon_geometry])\n",
    "#         img_union = n_images.overlay(n_images, how = 'union')\n",
    "        \n",
    "        if sum(temp_output['coverage']) < 1000:\n",
    "            cloud_lwr = 0\n",
    "            cloud_upr = 10\n",
    "            while sum(temp_output['coverage']) < 1000 and cloud_upr < 50 and len(temp_data) > len(temp_output):\n",
    "                temp_output = (\n",
    "                    pd.concat([temp_output,\n",
    "                               (temp_data[(temp_data['cloud_cover'] > cloud_lwr) &\n",
    "                                          (temp_data['cloud_cover'] <= cloud_upr)]\n",
    "                                # arrange in correct date order\n",
    "                                .join(date_order, on = ['region', 'year', 'date'])\n",
    "                                .sort_values(by = ['idx'])\n",
    "                                .reset_index(drop = True))])\n",
    "                    .reset_index(drop = True)\n",
    "                )\n",
    "                \n",
    "                cloud_lwr = cloud_lwr + 10\n",
    "                cloud_upr = cloud_upr + 10\n",
    "                \n",
    "        print('coverage: ' + str(sum(temp_output['coverage'])))\n",
    "        print('coverage - 1: ' + str(sum(temp_output.drop([temp_output.tail(1).index[0]])['coverage'])))\n",
    "        # check coverage - look for 1000% coverage over all images\n",
    "        while sum(temp_output.drop([temp_output.tail(1).index[0]])['coverage']) > 1000:\n",
    "            print('Removing index ' + str(temp_output.tail(1).index[0]))\n",
    "            temp_output = temp_output.drop([temp_output.tail(1).index[0]])\n",
    "            print('coverage - 1: ' + str(sum(temp_output.drop([temp_output.tail(1).index[0]])['coverage'])))\n",
    "\n",
    "\n",
    "        # add images to output\n",
    "        images_ordered = pd.concat([images_ordered, \n",
    "                                    temp_output])\n",
    "        print('\\n')\n",
    "\n",
    "images_ordered.reset_index()\n",
    "# images_ordered.to_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/yg_val_regions/planet_images_filtered.csv',\n",
    "#                       index = False)\n",
    "images_ordered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f121e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bit of clean-up\n",
    "images_ordered['coverage'] = pd.to_numeric(images_ordered['coverage'])\n",
    "len(images_ordered.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e53b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview thumbnails to remove remaining cloudy images\n",
    "thumbnails = [\n",
    "    feature['_links']['thumbnail'] \n",
    "    for search in metadata_df.metadata \n",
    "    for feature in search['features'] \n",
    "    if feature['id'] in [\n",
    "        str(image) \n",
    "        for image in images_ordered.id\n",
    " ]\n",
    "]\n",
    "print(len(thumbnails))\n",
    "\n",
    "thumbnails = remove_duplicates(thumbnails)\n",
    "print(len(thumbnails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcbe8ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # preview thumbnails to inspect\n",
    "# cloudy_imgs = []\n",
    "# for link in thumbnails:\n",
    "#     img_id = link.split('/')[8]\n",
    "#     print('Viewing thumbnail for ' + str(img_id))\n",
    "#     response = requests.get(link,\n",
    "#                     auth=HTTPBasicAuth(api_key, ''))\n",
    "#     thumbnail = Image.open(BytesIO(response.content))\n",
    "#     plt.imshow(thumbnail)\n",
    "#     plt.show()\n",
    "    \n",
    "#     # provide input about whether to download the image\n",
    "#     keep = input('Should this image be removed?(y/n)\\n')\n",
    "    \n",
    "#     # save user input with img_id to use as a filter later\n",
    "#     if keep == 'y':\n",
    "#         cloudy_imgs.append(img_id)\n",
    "        \n",
    "# cloudy_imgs = pd.DataFrame({'cloudy_imgs': cloudy_imgs})\n",
    "# cloudy_imgs.to_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/yg_val_regions/cloudy_images_to_remove.csv',\n",
    "#                        index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebd4fd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # filter images based on clouds and dates\n",
    "# images_filtered = pd.DataFrame(columns = potential_images.columns)\n",
    "# for region in aoi.index:\n",
    "#     print('###################################################')\n",
    "#     print(region)\n",
    "#     print('###################################################')\n",
    "    \n",
    "#     polygon_geometry = (\n",
    "#         aoi[region:region+1].reset_index()\n",
    "#         .rename(columns = {'index': 'region'})\n",
    "#         .loc[:, ['region','geometry']]\n",
    "#     )\n",
    "    \n",
    "#     for year in potential_images['year'].unique():\n",
    "#         print(year)\n",
    "        \n",
    "#         # get data\n",
    "#         temp_data = potential_images[(potential_images['region'] == region) &\n",
    "#                                      (potential_images['year'] == year) &\n",
    "#                                      (~potential_images.id.isin(list(cloudy_imgs.cloudy_imgs)))]\n",
    "        \n",
    "#         # first get all images with no clouds\n",
    "#         temp_output = (\n",
    "#             temp_data[(temp_data['cloud_cover'] == 0)]\n",
    "#             # arrange in correct date order\n",
    "#             .join(date_order, on = ['region', 'year', 'date'])\n",
    "#             .sort_values(by = ['idx'])\n",
    "#             .reset_index(drop = True)\n",
    "#         )\n",
    "        \n",
    "#         ### It would be nice to get an image count across the entire image and make sure each location\n",
    "#         ### has at least 10, but I haven't been able to figure out how\n",
    "# #         # check number of images\n",
    "# #         polygon_geometry['n_images'] = 0\n",
    "# #         n_images = temp_output.loc[:, ['region', 'geometry']]\n",
    "# #         n_images['n_images'] = 1\n",
    "# #         n_images = pd.concat([n_images, polygon_geometry])\n",
    "# #         img_union = n_images.overlay(n_images, how = 'union')\n",
    "        \n",
    "#         if sum(temp_output['coverage']) < 1000:\n",
    "#             cloud_lwr = 0\n",
    "#             cloud_upr = 10\n",
    "#             while sum(temp_output['coverage']) < 1000 and cloud_upr < 50 and len(temp_data) > len(temp_output):\n",
    "#                 temp_output = (\n",
    "#                     pd.concat([temp_output,\n",
    "#                                (temp_data[(temp_data['cloud_cover'] > cloud_lwr) &\n",
    "#                                           (temp_data['cloud_cover'] <= cloud_upr)]\n",
    "#                                 # arrange in correct date order\n",
    "#                                 .join(date_order, on = ['region', 'year', 'date'])\n",
    "#                                 .sort_values(by = ['idx'])\n",
    "#                                 .reset_index(drop = True))])\n",
    "#                     .reset_index(drop = True)\n",
    "#                 )\n",
    "                \n",
    "#                 cloud_lwr = cloud_lwr + 10\n",
    "#                 cloud_upr = cloud_upr + 10\n",
    "                \n",
    "#         print('coverage: ' + str(sum(temp_output['coverage'])))\n",
    "#         print('coverage - 1: ' + str(sum(temp_output.drop([temp_output.tail(1).index[0]])['coverage'])))\n",
    "#         # check coverage - look for 1000% coverage over all images\n",
    "#         while sum(temp_output.drop([temp_output.tail(1).index[0]])['coverage']) > 1000:\n",
    "#             print('Removing index ' + str(temp_output.tail(1).index[0]))\n",
    "#             temp_output = temp_output.drop([temp_output.tail(1).index[0]])\n",
    "#             print('coverage - 1: ' + str(sum(temp_output.drop([temp_output.tail(1).index[0]])['coverage'])))\n",
    "\n",
    "\n",
    "#         # add images to output\n",
    "#         images_filtered = pd.concat([images_filtered, \n",
    "#                                     temp_output])\n",
    "#         print('\\n')\n",
    "\n",
    "# images_filtered.reset_index()\n",
    "# # images_filtered.to_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/yg_val_regions/planet_images_filtered_manual_cloud_removal.csv',\n",
    "# #                       index = False)\n",
    "# # images_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a50187",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_filtered = pd.read_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/yg_val_regions/planet_images_filtered_manual_cloud_removal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6adfe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bit of clean-up\n",
    "images_filtered['coverage'] = pd.to_numeric(images_filtered['coverage'])\n",
    "len(images_filtered.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1b6ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bboxes for all images to visually inspect coverage\n",
    "bboxes = [\n",
    "    feature['geometry']['coordinates'][0]\n",
    "    for search in metadata_df.metadata \n",
    "    for feature in search['features'] \n",
    "    if feature['id'] in [\n",
    "        str(image) \n",
    "        for image in images_filtered.id\n",
    " ]\n",
    "]\n",
    "ids = [\n",
    "    feature['id']\n",
    "    for search in metadata_df.metadata \n",
    "    for feature in search['features'] \n",
    "    if feature['id'] in [\n",
    "        str(image) \n",
    "        for image in images_filtered.id\n",
    " ]\n",
    "]\n",
    "\n",
    "bboxes = [shp.geometry.Polygon([shp.geometry.Point(x, y) for x, y in feature]) for feature in bboxes]\n",
    "bboxes = gpd.GeoDataFrame(data = {'img_id': ids},\n",
    "                          crs = 'EPSG:4326',\n",
    "                          geometry = bboxes)\n",
    "# bboxes.to_file('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/yg_val_regions/image_footprints/planet_images_footprints.shp')\n",
    "bboxes\n",
    "# len(images_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd4efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of images that have no cloud cover by polygon and year\n",
    "image_counts_cloud_free = (\n",
    "    pd.DataFrame(images_filtered[images_filtered['cloud_cover'] == 0][['region', 'year']]\n",
    "    .groupby(['region', 'year'])\n",
    "    .value_counts())\n",
    "    .rename(columns = {0: 'cloud_free_image_count'})\n",
    ")\n",
    "image_counts_cloud_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad469ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 region has only 9 completely cloud free image in 2018\n",
    "pprint(image_counts_cloud_free[image_counts_cloud_free['cloud_free_image_count'] == min(image_counts_cloud_free['cloud_free_image_count'])])\n",
    "pd.DataFrame(image_counts_cloud_free[['cloud_free_image_count']]\n",
    "             .groupby('year')\n",
    "             .value_counts()\n",
    "             .sort_index()).rename(columns = {0: 'region_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b24dec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.displot(data = image_counts_cloud_free, \n",
    "            x = 'cloud_free_image_count',\n",
    "#             hue = 'year',\n",
    "#             multiple = 'stack',\n",
    "#             alpha = 0.5,\n",
    "            row = 'year'\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bed895",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get count of images by region and year\n",
    "image_counts_f1 = images_filtered[['region', 'year']].value_counts()\n",
    "image_counts_f1 = pd.DataFrame(image_counts_f1, columns = ['count']).sort_index()\n",
    "image_counts_f1 = (\n",
    "    image_counts_f1.join(\n",
    "        images_filtered[['region', 'year', 'coverage', 'area', 'cloud_cover']]\n",
    "        .groupby(['region', 'year'])\n",
    "        .aggregate({'coverage': 'sum',\n",
    "                    'area': 'sum',\n",
    "                    'cloud_cover': 'max'})\n",
    "        .rename(columns = {'coverage': 'cumulative_coverage',\n",
    "                           'area': 'cumulative_area',\n",
    "                           'cloud_cover': 'max_cloud_cover'})\n",
    "    ).rename(columns = {'count': 'img_count'})\n",
    ")\n",
    "image_counts_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e05832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40% is the max cloud cover in an image\n",
    "image_counts_f1[image_counts_f1['max_cloud_cover'] == max(image_counts_f1['max_cloud_cover'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65670ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total area if all of these images are downloaded\n",
    "images_filtered['planet_quota_usage'] = [area if area >= 100 else 100 for area in images_filtered.area]\n",
    "total_area = round(sum(images_filtered.area))\n",
    "print(total_area)\n",
    "print(round(total_area/5000000*100, ndigits = 2))\n",
    "quota_usage = round(sum(images_filtered.planet_quota_usage))\n",
    "print(quota_usage)\n",
    "print(round(quota_usage/5000000*100, ndigits = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f257ab9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# determine area that would be used if I didn't clip the images\n",
    "image_footprints = []\n",
    "for img_id in images_filtered.id:\n",
    "    image_footprints.append([shp.geometry.Polygon(feature['geometry']['coordinates'][0]) \n",
    "                             for metadata in metadata_df.metadata \n",
    "                             for feature in metadata['features']\n",
    "                             if feature['id'] == img_id][0])\n",
    "image_footprints = gpd.GeoDataFrame({'geometry': image_footprints},\n",
    "                                   crs = 'EPSG:4326')\n",
    "image_footprints = gpd.GeoDataFrame(pd.concat([images_filtered[['region', 'year', 'date', 'id']]\n",
    "                                               .reset_index(drop = True),\n",
    "                                               image_footprints],\n",
    "                                    axis = 1))\n",
    "\n",
    "area = pd.Series(dtype = 'float64')\n",
    "crs_list = ['EPSG:32642', 'EPSG:32642', 'EPSG:32643', 'EPSG:32643']\n",
    "for region in range(0, 4):\n",
    "    image_subset = image_footprints[image_footprints.region == region]\n",
    "    \n",
    "    # set crs to appropriate UTM zone\n",
    "    crs = crs_list[region]\n",
    "    image_subset = image_subset.to_crs(crs)\n",
    "    area = pd.concat([area, image_subset.geometry.area])\n",
    "    \n",
    "round(area.sum()/1e6/5000000*100, ndigits = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd41c80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.displot(data = image_counts_f1, \n",
    "            x = 'max_cloud_cover',\n",
    "#             hue = 'year',\n",
    "#             multiple = 'stack',\n",
    "#             alpha = 0.5,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319537a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
