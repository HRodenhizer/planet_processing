{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbddc0dd",
   "metadata": {},
   "source": [
    "# Planet Image Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e268a2ad",
   "metadata": {},
   "source": [
    "## API and Package Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ac87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from google.cloud import storage\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapely as shp\n",
    "from pprint import pprint\n",
    "import ast\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d286b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Planet API Key\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "api_key = os.getenv('PL_API_KEY')\n",
    "gcs_key = os.getenv('GCS_PL_ORDERS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up access to abrupt_thaw\n",
    "storage_client = storage.Client(project=\"AbruptThawMapping\")\n",
    "abrupt_thaw = storage_client.get_bucket('abrupt_thaw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cec214",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de42463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shapefile with AOI (multipolygon)\n",
    "aoi = gpd.read_file(\"/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/yg_val_regions/bboxes/yg_validation_bboxes.shp\")\n",
    "aoi['region'] = [0, 1, 2, 3]\n",
    "# convert from multipolygon to multiple polygons\n",
    "aoi = aoi.explode(column = 'geometry', ignore_index = True)\n",
    "# remove inner holes\n",
    "aoi.geometry = aoi.geometry.exterior\n",
    "pprint(aoi.geometry)\n",
    "# convert back to polygon\n",
    "aoi.geometry = [shp.geometry.Polygon([shp.geometry.Point(x, y) for x, y in list(feature.coords)]) for feature in aoi.geometry]\n",
    "pprint(aoi.geometry)\n",
    "# convert to json for planet data search\n",
    "sites = json.loads(aoi.to_json()) # if multiple sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1026bc6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import information on images to download\n",
    "images = pd.read_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/yg_val_regions/planet_images_filtered_manual_cloud_removal.csv')\n",
    "images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39728d01",
   "metadata": {},
   "source": [
    "## Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7dc3b",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# These values should start at 0 the first time through\n",
    "# if the code stops running due to a loss of connectivity, \n",
    "# change them to reflect the first region/chunk combination\n",
    "# you want to download\n",
    "start_region = 0\n",
    "start_chunk = 0\n",
    "    \n",
    "order_info_path = '/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/yg_val_regions/planet_image_orders.csv'\n",
    "try:\n",
    "    prior_orders = pd.read_csv(order_info_path,\n",
    "                               converters = {'order_info': ast.literal_eval})\n",
    "    prior_orders = [row['order_info'] for idx, row in prior_orders.iterrows()]\n",
    "    prior_items = [item['products'][0]['item_ids'] for item in prior_orders]\n",
    "except FileNotFoundError:\n",
    "    prior_items = []\n",
    "pprint(prior_items)\n",
    "    \n",
    "for region, site in enumerate(sites['features'][start_region:len(sites['features'])]):\n",
    "    \n",
    "    pprint(site)\n",
    "    \n",
    "    # set chunk to start on\n",
    "    if region + start_region > start_region:\n",
    "        start_chunk = 0\n",
    "    \n",
    "    # Set the delivery path\n",
    "    dir_path = (\n",
    "        \"planet_processing/data/yg_val_regions/planet_data/region_\"\n",
    "        + str(region + start_region) \n",
    "        + '/'\n",
    "    )\n",
    "\n",
    "    # Get ids for images that we want to request\n",
    "    all_item_ids = list(images[(images.region == region + start_region)].id)\n",
    "\n",
    "    # Get image ids for images that have already been downloaded\n",
    "    gcs_files = storage_client.list_blobs(\"abrupt_thaw\", prefix = dir_path)\n",
    "    current_files = []\n",
    "    for file in gcs_files:\n",
    "        if re.match('.*SR.*tif$', file.name):\n",
    "            current_files.append(file.name.split('/')[-1].split('_3B_Analytic')[0])\n",
    "    \n",
    "    # Filter images to download to avoid repeat downloads\n",
    "    items_to_download = [i for i in all_item_ids if i not in current_files]\n",
    "    \n",
    "    pprint(items_to_download)\n",
    "\n",
    "    print(\"-------------------------------------\")\n",
    "    print(\"\\n\")\n",
    "    print(len(items_to_download), \"items to download for region \" + str(region + start_region))\n",
    "    print(\"\\n\")\n",
    "    print(\"-------------------------------------\")\n",
    "\n",
    "    # downloading items in defined chunks -- chunks with fewer images (<10 works better than downloading each chunk with many images\n",
    "\n",
    "    if len(items_to_download) > 0: # if images to download\n",
    "        \n",
    "        if len(items_to_download) < 5:\n",
    "            img_chunks = [items_to_download]\n",
    "        else:\n",
    "            n_items = 5\n",
    "            n_chunks = int(len(items_to_download) / n_items)\n",
    "            img_chunks = np.array_split(items_to_download, n_chunks)\n",
    "            img_chunks = [list(feature) for feature in img_chunks]\n",
    "        \n",
    "        for chunk_idx, item_ids in enumerate(img_chunks):\n",
    "            print('Chunk', chunk_idx + start_chunk)\n",
    "            print('Item IDs:', item_ids, '\\n')\n",
    "\n",
    "            if item_ids not in prior_items: # check if an order has already been placed\n",
    "                now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                chunk_id = 'YG_validation_region' + str(region + start_region) + '_chunk' + str(chunk_idx + start_chunk) + '_' + now\n",
    "\n",
    "                # create the order info\n",
    "                order_info = {\n",
    "                    \"name\": chunk_id,\n",
    "                    \"source_type\": \"scenes\",\n",
    "                    \"products\": [{\n",
    "                        \"item_ids\": item_ids,\n",
    "                        \"item_type\": \"PSScene\",\n",
    "                        \"product_bundle\": \"analytic_sr_udm2,analytic_8b_sr_udm2\"\n",
    "                    }],\n",
    "                    \"tools\": [{\n",
    "                        \"clip\": {\n",
    "                            \"aoi\": {\n",
    "                                    \"type\": \"Polygon\",\n",
    "                                    \"coordinates\": site['geometry']['coordinates']\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"harmonize\": {\n",
    "                            \"target_sensor\": \"Sentinel-2\"\n",
    "                        }\n",
    "                    }],\n",
    "                    \"delivery\": {\n",
    "                        \"google_cloud_storage\": {\n",
    "                            \"bucket\":\"abrupt_thaw\",\n",
    "                            \"credentials\": gcs_key,\n",
    "                            \"path_prefix\": dir_path\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # send request to Planet\n",
    "                request = requests.post('https://api.planet.com/compute/ops/orders/v2', \n",
    "                                        auth=(api_key, ''),\n",
    "                                        json=order_info)\n",
    "                print(request)\n",
    "                pprint(request.json())\n",
    "                \n",
    "                # wait while the order is queued and runs\n",
    "                order_status = request.json()\n",
    "                while order_status['state'] == 'queued':\n",
    "                    time.sleep(1)\n",
    "                    order_status = requests.get(request.json()['_links']['_self'], \n",
    "                                                auth=(api_key, '')).json()\n",
    "                    \n",
    "                while order_status['state'] == 'running':\n",
    "                    time.sleep(1)\n",
    "                    order_status = requests.get(request.json()['_links']['_self'], \n",
    "                                                auth=(api_key, '')).json()\n",
    "                \n",
    "                # If the order succeeded, create and append order info into file\n",
    "                if order_status['state'] == 'success':\n",
    "                    order_df = pd.DataFrame({\n",
    "                        'region': region + start_region,\n",
    "                        'chunk_idx': chunk_idx + start_chunk,\n",
    "                        'order_name': chunk_id,\n",
    "                        'order_info': [request.json()]\n",
    "                    })\n",
    "\n",
    "                    order_df.to_csv(order_info_path,\n",
    "                                    index = False,\n",
    "                                    mode = 'a',\n",
    "                                    header = not os.path.exists(order_info_path))\n",
    "                \n",
    "                # If the order failed, stop the code from running any farther\n",
    "                elif order_status['state'] == 'failed':\n",
    "                    print('last_message:', order_status['last_message'], '\\nerror_hints:', order_status['error_hints'])\n",
    "                    break\n",
    "    \n",
    "            else:\n",
    "                print('Order already placed.')\n",
    "    \n",
    "        if order_status['state'] == 'failed':\n",
    "            break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d91f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In case I need to cancel orders quickly\n",
    "# requests.post('https://api.planet.com/compute/ops/bulk/orders/v2/cancel',\n",
    "#               auth=(api_key, ''))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
