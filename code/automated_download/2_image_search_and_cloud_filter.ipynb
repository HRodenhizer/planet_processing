{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf9fe9a",
   "metadata": {},
   "source": [
    "# Planet Image Search and MODIS Cloud Cover Filter\n",
    "### This script searches Planet PSScene imager and filters the search results by cloud cover using MODIS and Planet derived cloud cover metrics.\n",
    "#### TODO:\n",
    "- use clear_percent instead of cloud_percent to ensure that all haze and shadows are accounted for\n",
    "- figure out how to ensure coverage in all cells\n",
    "- Don't just remove all images <1.5 km^2, check for neighbors first. If it has a neighbor, the images can be merged after download and prior to processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3197feca",
   "metadata": {},
   "source": [
    "## API and Package Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb68e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Earth Engine API\n",
    "import ee\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d783ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import geemap\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapely as shp\n",
    "from pprint import pprint\n",
    "import ast\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f003ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Planet API Key\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "api_key = os.getenv('PL_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dda253",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c91db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import MODIS Data\n",
    "# AOI\n",
    "yg = ee.Geometry({\n",
    "    'type': 'Polygon',\n",
    "    'coordinates': [[\n",
    "        [65, 65],\n",
    "        [65, 74],\n",
    "        [85, 74],\n",
    "        [85, 65],\n",
    "        [65, 65]\n",
    "        ]]\n",
    "})\n",
    "\n",
    "# MODIS data\n",
    "modis = ee.ImageCollection('MODIS/061/MOD09GA');\n",
    "\n",
    "# MODIS snow\n",
    "modis_snow = ee.ImageCollection('MODIS/006/MOD10A1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad44ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shapefile with AOI (multipolygon)\n",
    "aoi = gpd.read_file(\"/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/automated_download/rts_buffer/RTS_buffer.shp\")\n",
    "# convert from multipolygon to multiple polygons\n",
    "aoi = aoi.explode(column = 'geometry', ignore_index = True)\n",
    "# remove inner holes\n",
    "aoi.geometry = aoi.geometry.exterior\n",
    "pprint(aoi.geometry)\n",
    "# convert back to polygon\n",
    "aoi.geometry = [shp.geometry.Polygon([shp.geometry.Point(x, y) for x, y in list(feature.coords)]) for feature in aoi.geometry]\n",
    "pprint(aoi.geometry)\n",
    "# convert to json for planet data search\n",
    "sites = json.loads(aoi.to_json()) # if multiple sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in aoi.iterrows():\n",
    "    data = aoi[index:index+1]\n",
    "    name = '/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/automated_download/rts_buffer/RTS_buffer_separate_' + str(index) + '.shp'\n",
    "    data.to_file(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae936300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Years to test\n",
    "years = [2017, 2018, 2019, 2020, 2021]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1df64f1",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aef3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to extract specific bits from bitmask\n",
    "def bitwiseExtract(input, fromBit, toBit):\n",
    "    maskSize = ee.Number(1).add(toBit).subtract(fromBit)\n",
    "    mask = ee.Number(1).leftShift(maskSize).subtract(1)\n",
    "    return input.rightShift(fromBit).bitwiseAnd(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c904d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to get snow data from MODIS\n",
    "def modisSnow(modis_snow_imagery, date, aoi):\n",
    "    # extract NDSI\n",
    "    snow_cover = (ee.Image(modis_snow_imagery\n",
    "                           .filter(ee.Filter.date(date))\n",
    "                           .select(['NDSI_Snow_Cover'])\n",
    "                           .first())\n",
    "                  .clip(aoi));\n",
    "    \n",
    "    # get average snow cover\n",
    "    snow_cover = snow_cover.reduceRegion(ee.Reducer.max(), aoi);\n",
    "    \n",
    "    return snow_cover.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18654c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to extract metadata needed for cloud calculation\n",
    "def getMetadata(feature, aoi, polygon_id):\n",
    "    \n",
    "    # get image id\n",
    "    img_id = feature['id']\n",
    "    \n",
    "    # get image date\n",
    "    img_date = feature['properties']['acquired'].split('T')[0]\n",
    "    \n",
    "    # get instrument type\n",
    "    instrument_type = feature['properties']['instrument']\n",
    "    \n",
    "    # get planet cloud/cloud shadow cover\n",
    "    if instrument_type == 'PS2':\n",
    "        img_cloud_cover = float(feature['properties']['cloud_cover']*100)\n",
    "    else:\n",
    "        img_cloud_cover = float(feature['properties']['cloud_percent'] + feature['properties']['shadow_percent'])\n",
    "    \n",
    "    # use intersection of aoi and search result geometry to get actual geometry of cells with data\n",
    "    img_geometry = (\n",
    "        aoi[polygon_id:polygon_id+1]['geometry']\n",
    "        .intersection(\n",
    "            shp.geometry.Polygon(\n",
    "                tuple([(feature[0], feature[1]) for feature in feature['geometry']['coordinates'][0]])\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # get image area\n",
    "    img_area = float(img_geometry.to_crs(crs = 32642).area/1e6)\n",
    "    \n",
    "    # get image coverage of AOI polygon\n",
    "    img_coverage = round(float(img_geometry.to_crs(crs = 32642).area/aoi[polygon_id:polygon_id+1]['geometry'].to_crs(crs = 32642).area*100))\n",
    "    \n",
    "    return [img_id, img_date, instrument_type, img_coverage, img_area, img_cloud_cover, img_geometry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df3fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometryToEE(img_geometry, polygon_id):\n",
    "    \n",
    "    # format geometry\n",
    "    img_geometry = gpd.GeoDataFrame(geometry = img_geometry)\n",
    "    img_geometry = [[[x, y] for x, y in list(img_geometry.geometry[polygon_id].exterior.coords)]]\n",
    "    \n",
    "    # convert geometry to ee.Geometry\n",
    "    img_geometry_ee = ee.Geometry({\n",
    "        'type': 'Polygon',\n",
    "        'coordinates': img_geometry\n",
    "    })\n",
    "    \n",
    "    return img_geometry_ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to calculate MODIS cloud cover\n",
    "def modisCloudCover(modis_imagery, date, aoi):\n",
    "    # extract QC bitmask band from MODIS\n",
    "    qc = (ee.Image(modis_imagery\n",
    "              .filter(ee.Filter.date(date))\n",
    "              .first())\n",
    "      .select(['state_1km'])\n",
    "      .clip(aoi));\n",
    "    \n",
    "    # extract cloud information from MODIS QC bitmask\n",
    "    cloud_mask = bitwiseExtract(qc, 0, 1).remap([0, 1, 2, 3], [0, 1, 1, 1])\n",
    "    area_mask = cloud_mask.remap([0, 1], [1, 1])\n",
    "    \n",
    "    # calculate area of cells with clouds\n",
    "    cloud_area_img = cloud_mask.multiply(ee.Image.pixelArea())\n",
    "    area_img = area_mask.multiply(ee.Image.pixelArea())\n",
    "    \n",
    "    # calculate cloud cover percent\n",
    "    cloud_area = (\n",
    "        cloud_area_img\n",
    "        .reduceRegion(\n",
    "            reducer = ee.Reducer.sum(), # calculate total cloud area\n",
    "            geometry = aoi,\n",
    "            scale = 1000,\n",
    "            maxPixels = 1e10\n",
    "        )\n",
    "        .getNumber('remapped')\n",
    "        .divide(\n",
    "            area_img\n",
    "            .reduceRegion(\n",
    "                reducer = ee.Reducer.sum(), # calculate total area\n",
    "                geometry = aoi,\n",
    "                scale = 1000,\n",
    "                maxPixels = 1e10\n",
    "            )\n",
    "            .getNumber('remapped')\n",
    "        ) # divide cloud area by total area\n",
    "        .multiply(ee.Number(100)) # convert to %\n",
    "        .round() # remove decimal precision\n",
    "    );\n",
    "    \n",
    "    return cloud_area.getInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2abe2a9",
   "metadata": {},
   "source": [
    "## Determine Snow Free Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e1e47f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# modis_snow_df = pd.DataFrame(columns = ['polygon_id', 'year', 'date', 'snow_cover'])\n",
    "# for index, row in aoi.iterrows():\n",
    "#     print(index)\n",
    "#     geometry = [[[x, y] for x, y in list(row.geometry.exterior.coords)]]\n",
    "    \n",
    "#     # convert geometry to ee.Geometry\n",
    "#     geometry_ee = ee.Geometry({\n",
    "#         'type': 'Polygon',\n",
    "#         'coordinates': geometry\n",
    "#     })\n",
    "#     for year in years:\n",
    "#         print(year)\n",
    "#         dates = pd.date_range(str(year) + '-06-01', str(year) + '-08-31', freq = 'D')\n",
    "        \n",
    "#         for date in dates:\n",
    "            \n",
    "#             snow = modisSnow(modis_snow, date, geometry_ee)\n",
    "            \n",
    "#             # add to output\n",
    "#             modis_snow_df = pd.concat([modis_snow_df,\n",
    "#                                        pd.DataFrame({'polygon_id': index,\n",
    "#                                                      'year': year,\n",
    "#                                                      'date': date,\n",
    "#                                                      'snow_cover': snow})])\n",
    "\n",
    "# modis_snow_df = modis_snow_df.fillna(value = np.NAN)\n",
    "# modis_snow_df = modis_snow_df.reset_index(drop = True)\n",
    "# modis_snow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a6d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modis_snow_df.to_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/automated_download/modis_snow_data.csv',\n",
    "#                     index = False)\n",
    "modis_snow_df = pd.read_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/automated_download/modis_snow_data.csv')\n",
    "modis_snow_df['date'] = pd.to_datetime(modis_snow_df['date'])\n",
    "modis_snow_df = modis_snow_df.dropna(how = 'any').reset_index()\n",
    "modis_snow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7673480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define first snow-free date as first of three consecutive NDSI == 0 (removing NaN values)\n",
    "modis_snow_df['snow_free'] = np.where(modis_snow_df.snow_cover == 0, 1, 0)\n",
    "modis_snow_df['rolling_snow_free'] = (\n",
    "    modis_snow_df\n",
    "    .groupby(['polygon_id', 'year'])\n",
    "    .snow_free\n",
    "    .rolling(3).sum().shift(-2)\n",
    "    .reset_index(drop = True)\n",
    ")\n",
    "snow_free_date = (\n",
    "    modis_snow_df[modis_snow_df.rolling_snow_free == 3]\n",
    "    .groupby(['polygon_id', 'year'])\n",
    "    .first()\n",
    "    .rename(columns = {'date': 'snow_free_date'})\n",
    "    .snow_free_date\n",
    ")\n",
    "snow_free_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c14bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_snow_df[(modis_snow_df.polygon_id == 27) & \n",
    "              (modis_snow_df.date.between('2019-06-10', '2019-07-20'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08119a3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # import warnings\n",
    "# with warnings.catch_warnings(): # there is a warning getting triggered inside of sns, I think\n",
    "#     warnings.simplefilter(\"ignore\")\n",
    "#     g = sns.FacetGrid(data = modis_snow_df,\n",
    "#                           col = 'year',\n",
    "#                           row = 'polygon_id',\n",
    "#                           sharex = False)\n",
    "#     g.map(sns.lineplot, 'date', 'snow_cover')\n",
    "    \n",
    "#     for ax, pos in zip(g.axes.flat, snow_free_date):\n",
    "#         ax.axvline(x=pos, color='black', linestyle=':')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f8c15",
   "metadata": {},
   "source": [
    "## Search Planet Imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56deeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_id = 0\n",
    "year = 2017\n",
    "start_date = snow_free_date.loc[(polygon_id, year)]\n",
    "start_date < pd.to_datetime('2017-07-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf265e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_metadata = []\n",
    "# metadata_df = pd.DataFrame(columns = ['polygon_id', 'year', 'metadata'])\n",
    "\n",
    "# # Data type\n",
    "# item_type = \"PSScene\"\n",
    "\n",
    "# # asset filter\n",
    "# asset_filter = {\n",
    "#     'type': 'OrFilter',\n",
    "#     'config': [\n",
    "#        {\n",
    "#            \"type\": \"AndFilter\",\n",
    "#             \"config\": [\n",
    "#                 {\n",
    "#                     \"type\": \"AssetFilter\",\n",
    "#                     \"config\": [\n",
    "#                         \"ortho_analytic_4b_sr\"\n",
    "#                     ]\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"type\": \"AssetFilter\",\n",
    "#                     \"config\": [\n",
    "#                         \"ortho_udm2\"\n",
    "#                     ]\n",
    "#                 }\n",
    "#             ]\n",
    "#         },\n",
    "#         {\n",
    "#             \"type\": \"AndFilter\",\n",
    "#             \"config\": [\n",
    "#                 {\n",
    "#                     \"type\": \"AssetFilter\",\n",
    "#                     \"config\": [\n",
    "#                         \"ortho_analytic_8b_sr\"\n",
    "#                     ]\n",
    "#                 },\n",
    "#                 {\n",
    "#                     \"type\": \"AssetFilter\",\n",
    "#                     \"config\": [\n",
    "#                         \"ortho_udm2\"\n",
    "#                     ]\n",
    "#                 }\n",
    "#             ]\n",
    "#         } \n",
    "#     ]\n",
    "    \n",
    "# }\n",
    "\n",
    "# for polygon_id, site in enumerate(sites['features']):\n",
    "    \n",
    "#     print(site['id'])\n",
    "#     site_name = site['id']\n",
    "\n",
    "#     session = requests.Session()\n",
    "#     session.auth = (api_key, '')\n",
    "#     site_coords = site['geometry']['coordinates']\n",
    "\n",
    "#     site_dict = {\n",
    "#         \"type\": \"Polygon\",\n",
    "#         \"coordinates\": site_coords}\n",
    "\n",
    "#     # get images that overlap with our aoi\n",
    "#     geometry_filter = {\n",
    "#         \"type\": \"GeometryFilter\",\n",
    "#         \"field_name\": \"geometry\",\n",
    "#         \"config\": site_dict\n",
    "#     }\n",
    "\n",
    "#     for year in years:\n",
    "        \n",
    "#         if snow_free_date.loc[(polygon_id, year)] < pd.to_datetime('{}-07-01'.format(year)):\n",
    "#             start_date = str(snow_free_date.loc[(polygon_id, year)])[0:10]\n",
    "#         else:\n",
    "#             start_date = '{}-07-01'.format(year)\n",
    "\n",
    "#         # i only want images between these two dates of each year...easier to search within a year to avoid massive search queries\n",
    "#         start_date = \"{}T00:00:00.000Z\".format(start_date)\n",
    "#         end_date = \"{}-08-31T00:00:00.000Z\".format(year)\n",
    "\n",
    "#         # get images acquired within a date range\n",
    "#         date_range_filter = {\n",
    "#             \"type\": \"DateRangeFilter\",\n",
    "#             \"field_name\": \"acquired\",\n",
    "#             \"config\": {\n",
    "#                 \"gte\": start_date,\n",
    "#                 \"lte\": end_date\n",
    "#             }\n",
    "#         }\n",
    "\n",
    "#         cloud_cover_filter = {\n",
    "#             \"type\": \"RangeFilter\",\n",
    "#             \"field_name\": \"cloud_cover\",\n",
    "#             \"config\": {\n",
    "#                 \"lte\": 1 # cloud cover threshold - none currently\n",
    "#             }\n",
    "#         }\n",
    "\n",
    "#         combined_filter = {\n",
    "#             \"type\": \"AndFilter\",\n",
    "#             \"config\": [\n",
    "#                 asset_filter,\n",
    "# #                 instrument_filter,\n",
    "#                 geometry_filter,\n",
    "#                 date_range_filter,\n",
    "#                 cloud_cover_filter\n",
    "#             ]\n",
    "#         }\n",
    "\n",
    "#         # API request object\n",
    "#         search_request = {\n",
    "#             \"item_types\": [item_type],\n",
    "#             \"filter\": combined_filter\n",
    "#         }\n",
    "\n",
    "#         # fire off the POST request\n",
    "#         search_result = \\\n",
    "#           requests.post(\n",
    "#             'https://api.planet.com/data/v1/quick-search',\n",
    "#             auth=HTTPBasicAuth(api_key, ''),\n",
    "#             json=search_request)\n",
    "\n",
    "#         all_metadata.append(search_result.json())\n",
    "        \n",
    "#         # format metadata for dataframe\n",
    "#         temp_df = pd.DataFrame({'polygon_id': site_name,\n",
    "#                                 'year': year,\n",
    "#                                 'metadata': [search_result.json()]})\n",
    "        \n",
    "#         metadata_df = pd.concat([metadata_df, temp_df], axis = 0)\n",
    "# pprint('# of searches: ' + str(len(all_metadata)) + ' (should be ' + str(len(sites['features'])*len(years)) + ')')\n",
    "# metadata_df = metadata_df.reset_index(drop = True)\n",
    "# metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e82cfd4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # save output\n",
    "# metadata_df.to_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/automated_download/planet_image_search.csv')\n",
    "metadata_df = pd.read_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/automated_download/planet_image_search.csv',\n",
    "                          index_col=0,\n",
    "                          converters = {'metadata': ast.literal_eval})\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13791aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "[feature for feature in metadata_df.metadata[1]['features'] if feature['id'] == '20180722_064609_1032']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb127ba",
   "metadata": {},
   "source": [
    "## Calculate Cloud Cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e1eda5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cloud_data = pd.DataFrame(\n",
    "#     columns = [\n",
    "#         'polygon_id', \n",
    "#         'year', \n",
    "#         'date', \n",
    "#         'id',\n",
    "#         'instrument',\n",
    "#         'coverage', \n",
    "#         'area', \n",
    "#         'modis_cloud_cover', \n",
    "#         'planet_cloud_cover', \n",
    "#         'cloud_cover', \n",
    "#         'modis_planet_diff',\n",
    "#         'geometry']\n",
    "# )\n",
    "# for index,row in metadata_df.iterrows():\n",
    "    \n",
    "#     print(index);\n",
    "    \n",
    "#     # get metadata\n",
    "#     polygon_id = int(row['polygon_id']);\n",
    "#     img_year = int(row['year']);\n",
    "#     metadata = row['metadata']['features'];\n",
    "    \n",
    "#     if len(metadata) > 0: \n",
    "#         for feature in metadata:\n",
    "            \n",
    "#             # extract metadata needed for cloud cover calculations\n",
    "#             img_id, img_date, instrument_type, img_coverage, img_area, img_cloud_cover, img_geometry = getMetadata(feature, aoi, polygon_id);\n",
    "# #             print(img_id)\n",
    "            \n",
    "#             # calculate modis cloud cover if the geometry is a polygon\n",
    "#             if str(type(img_geometry[polygon_id])) == \"<class 'shapely.geometry.polygon.Polygon'>\":\n",
    "#                 # convert geometry to ee.Geometry\n",
    "#                 img_geometry_ee = geometryToEE(img_geometry, polygon_id);\n",
    "\n",
    "#                 # calc modis cloud cover\n",
    "#                 modis_cloud_cover = modisCloudCover(modis, img_date, img_geometry_ee);\n",
    "#                 if modis_cloud_cover > 100:\n",
    "#                     modis_cloud_cover = 100;\n",
    "                \n",
    "#             # calculate modis cloud cover if the geometry is a multipolygon\n",
    "#             elif str(type(img_geometry[polygon_id])) == \"<class 'shapely.geometry.multipolygon.MultiPolygon'>\":\n",
    "                \n",
    "#                 polygon_geometries = gpd.GeoDataFrame(\n",
    "#                     geometry = img_geometry\n",
    "#                 ).explode(column = 'geometry', ignore_index = True)\n",
    "#                 modis_cloud_cover = []\n",
    "#                 polygon_area = []\n",
    "#                 for index, row in polygon_geometries.iterrows():\n",
    "                    \n",
    "#                     # calculate area of sub polygon\n",
    "#                     temp_area = gpd.GeoDataFrame(\n",
    "#                         geometry = row, \n",
    "#                         crs = polygon_geometries.crs\n",
    "#                     ).reset_index().to_crs(crs = 32642).geometry.area/1e6\n",
    "#                     polygon_area.append(temp_area)\n",
    "                    \n",
    "#                     # convert geometry to ee.Geometry\n",
    "#                     polygon_geometry = [[[x, y] for x, y in list(row.geometry.exterior.coords)]]\n",
    "    \n",
    "#                     # convert geometry to ee.Geometry\n",
    "#                     polygon_geometry_ee = ee.Geometry({\n",
    "#                         'type': 'Polygon',\n",
    "#                         'coordinates': polygon_geometry\n",
    "#                     })\n",
    "\n",
    "#                     # calc modis cloud cover of sub polygon\n",
    "#                     temp_cloud_cover = modisCloudCover(modis, img_date, polygon_geometry_ee);\n",
    "#                     modis_cloud_cover.append(temp_cloud_cover)\n",
    "                    \n",
    "#                 # calculate average cloud cover of multipolygon (img_geometry)\n",
    "#                 modis_cloud_cover = int(round(sum(\n",
    "#                     [cloud * area for cloud, area in zip(modis_cloud_cover, polygon_area)]\n",
    "#                 )/img_area))\n",
    "#                 if modis_cloud_cover > 100:\n",
    "#                     modis_cloud_cover = 100;\n",
    "                    \n",
    "#             # get higher cloud cover estimate\n",
    "#             cloud_cover = np.maximum(modis_cloud_cover, img_cloud_cover)\n",
    "            \n",
    "#             # calculate difference between cloud cover estimates\n",
    "#             modis_planet_diff = abs(\n",
    "#                 modis_cloud_cover - img_cloud_cover\n",
    "#             )\n",
    "            \n",
    "#             # organize data into a dataframe\n",
    "#             temp_df = pd.DataFrame({\n",
    "#                 'polygon_id': polygon_id,\n",
    "#                 'year': img_year,\n",
    "#                 'date': img_date,\n",
    "#                 'id': img_id,\n",
    "#                 'instrument': instrument_type,\n",
    "#                 'coverage': img_coverage,\n",
    "#                 'area': img_area,\n",
    "#                 'modis_cloud_cover': modis_cloud_cover,\n",
    "#                 'planet_cloud_cover': img_cloud_cover,\n",
    "#                 'cloud_cover': cloud_cover,\n",
    "#                 'modis_planet_diff': modis_planet_diff,\n",
    "#                 'geometry': img_geometry\n",
    "#             });\n",
    "\n",
    "#             # append new data to old\n",
    "#             cloud_data = pd.concat([cloud_data, temp_df], axis = 0);\n",
    "\n",
    "# cloud_data = gpd.GeoDataFrame(cloud_data);\n",
    "# cloud_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9996972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # save output\n",
    "# cloud_data.to_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/automated_download/planet_images_modis_cloud.csv')\n",
    "cloud_data = pd.read_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/automated_download/planet_images_modis_cloud.csv',\n",
    "                  index_col = 0)\n",
    "cloud_data = gpd.GeoDataFrame(cloud_data,\n",
    "                              geometry = gpd.GeoSeries.from_wkt(cloud_data['geometry']),\n",
    "                              crs = 'EPSG:4326')\n",
    "cloud_data['date'] = pd.to_datetime(cloud_data['date'])\n",
    "cloud_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f06c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data = cloud_data, \n",
    "            x = 'area',\n",
    "#             hue = 'year',\n",
    "#             multiple = 'stack',\n",
    "#             alpha = 0.5,\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc151bb",
   "metadata": {},
   "source": [
    "## Filter Images on Cloud Cover and Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ce74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get count of images by polygon and year\n",
    "image_counts = cloud_data[['polygon_id', 'year']].value_counts()\n",
    "image_counts = pd.DataFrame(image_counts, columns = ['count']).sort_index()\n",
    "# image_counts.to_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/automated_download/image_counts_pre_filter.csv')\n",
    "image_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024972e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter by area >= 1.5 km^2 to ensure at least a few tie points for AROSICS\n",
    "# Don't filter on cloud cover yet?\n",
    "potential_images = cloud_data[cloud_data['area'] >= 1.5]\n",
    "potential_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7202c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use this to preview images that have extremely different cloud estimates from the two methods\n",
    "# it seems like most of the time, if one of the two estimates is high, there are a lot of clouds\n",
    "# however, a few images are mostly clear that MODIS says are cloudy\n",
    "# If there aren't enough images, this could be a place to manually change the cloud_cover\n",
    "potential_images[potential_images['modis_planet_diff'] > 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b55747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of dates in order of preference which will be used to sort the dataframe so that we can slice it to get image dates\n",
    "dates = pd.DataFrame({'date': pd.date_range(start = min(snow_free_date),\n",
    "                                                    end = '2017-08-31',\n",
    "                                                    freq = 'D').strftime('%Y-%m-%d')})\n",
    "np.where([bool(re.search('07-31', date)) for date in dates.date])[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5b5e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create list of dates in order of preference which will be used to sort the dataframe so that we can slice it to get image dates\n",
    "date_order = pd.DataFrame()\n",
    "for polygon_id in aoi.index:\n",
    "    for year in potential_images['year'].unique():\n",
    "        if snow_free_date[polygon_id, year] <= pd.Timestamp(str(year) + '-07-01'):\n",
    "            start_date = snow_free_date[polygon_id, year]\n",
    "        else:\n",
    "            start_date = pd.Timestamp(str(year) + '-07-01')\n",
    "        dates = pd.DataFrame({'date': pd.date_range(start = start_date,\n",
    "                                                    end = str(year) + '-08-31',\n",
    "                                                    freq = 'D').strftime('%Y-%m-%d')})\n",
    "        split_loc = np.where([bool(re.search('08-01', date)) for date in dates.date])[0][0]\n",
    "        dates_1 = np.flip(np.arange(0, split_loc))\n",
    "        dates_2 = np.arange(split_loc, len(dates))\n",
    "        idx = list(np.insert(dates_1, np.arange(0, len(dates_2)), dates_2))\n",
    "        dates = dates.iloc[idx].reset_index(drop = True).rename(columns = {0: 'date'})\n",
    "        dates['polygon_id'] = polygon_id\n",
    "        dates['year'] = year\n",
    "        dates['idx'] = dates.index.astype('Int32')\n",
    "        dates['date'] = pd.to_datetime(dates['date'])\n",
    "        dates = dates.set_index(['polygon_id', 'year', 'date'])\n",
    "        date_order = pd.concat([date_order, dates])\n",
    "date_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392568dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter images based on clouds and dates\n",
    "images_ordered = pd.DataFrame(columns = potential_images.columns)\n",
    "for polygon_id in aoi.index:\n",
    "    print('###################################################')\n",
    "    print(polygon_id)\n",
    "    print('###################################################')\n",
    "    \n",
    "    polygon_geometry = (\n",
    "        aoi[polygon_id:polygon_id+1].reset_index()\n",
    "        .rename(columns = {'index': 'polygon_id'})\n",
    "        .loc[:, ['polygon_id','geometry']]\n",
    "    )\n",
    "    \n",
    "    for year in potential_images['year'].unique():\n",
    "        print(year)\n",
    "        \n",
    "        # get data\n",
    "        temp_data = potential_images[(potential_images['polygon_id'] == polygon_id) &\n",
    "                                     (potential_images['year'] == year)]\n",
    "        \n",
    "        # first get all images with no clouds\n",
    "        temp_output = (\n",
    "            temp_data[(temp_data['cloud_cover'] == 0)]\n",
    "            # arrange in correct date order\n",
    "            .join(date_order, on = ['polygon_id', 'year', 'date'])\n",
    "            .sort_values(by = ['idx'])\n",
    "            .reset_index(drop = True)\n",
    "        )\n",
    "        \n",
    "        ### It would be nice to get an image count across the entire image and make sure each location\n",
    "        ### has at least 10, but I haven't been able to figure out how\n",
    "#         # check number of images\n",
    "#         polygon_geometry['n_images'] = 0\n",
    "#         n_images = temp_output.loc[:, ['polygon_id', 'geometry']]\n",
    "#         n_images['n_images'] = 1\n",
    "#         n_images = pd.concat([n_images, polygon_geometry])\n",
    "#         img_union = n_images.overlay(n_images, how = 'union')\n",
    "        \n",
    "        if sum(temp_output['coverage']) < 1000:\n",
    "            cloud_lwr = 0\n",
    "            cloud_upr = 10\n",
    "            while sum(temp_output['coverage']) < 1000 and cloud_upr < 50 and len(temp_data) > len(temp_output):\n",
    "                temp_output = (\n",
    "                    pd.concat([temp_output,\n",
    "                               (temp_data[(temp_data['cloud_cover'] > cloud_lwr) &\n",
    "                                          (temp_data['cloud_cover'] <= cloud_upr)]\n",
    "                                # arrange in correct date order\n",
    "                                .join(date_order, on = ['polygon_id', 'year', 'date'])\n",
    "                                .sort_values(by = ['idx'])\n",
    "                                .reset_index(drop = True))])\n",
    "                    .reset_index(drop = True)\n",
    "                )\n",
    "                \n",
    "                cloud_lwr = cloud_lwr + 10\n",
    "                cloud_upr = cloud_upr + 10\n",
    "                \n",
    "        print('coverage: ' + str(sum(temp_output['coverage'])))\n",
    "        print('coverage - 1: ' + str(sum(temp_output.drop([temp_output.tail(1).index[0]])['coverage'])))\n",
    "        # check coverage - look for 1000% coverage over all images\n",
    "        while sum(temp_output.drop([temp_output.tail(1).index[0]])['coverage']) > 1000:\n",
    "            print('Removing index ' + str(temp_output.tail(1).index[0]))\n",
    "            temp_output = temp_output.drop([temp_output.tail(1).index[0]])\n",
    "            print('coverage - 1: ' + str(sum(temp_output.drop([temp_output.tail(1).index[0]])['coverage'])))\n",
    "\n",
    "\n",
    "        # add images to output\n",
    "        images_ordered = pd.concat([images_ordered, \n",
    "                                    temp_output])\n",
    "        print('\\n')\n",
    "\n",
    "images_ordered.reset_index()\n",
    "# images_ordered.to_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/automated_download/planet_images_filtered.csv',\n",
    "#                       index = False)\n",
    "images_ordered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f121e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bit of clean-up\n",
    "images_ordered['coverage'] = pd.to_numeric(images_ordered['coverage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd4efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of images that have no cloud cover by polygon and year\n",
    "image_counts_cloud_free = (\n",
    "    pd.DataFrame(images_ordered[images_ordered['cloud_cover'] == 0][['polygon_id', 'year']]\n",
    "    .groupby(['polygon_id', 'year'])\n",
    "    .value_counts())\n",
    "    .rename(columns = {0: 'cloud_free_image_count'})\n",
    ")\n",
    "image_counts_cloud_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad469ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 RTS Polygon has only 3 completely cloud free image in 2018\n",
    "pprint(image_counts_cloud_free[image_counts_cloud_free['cloud_free_image_count'] == min(image_counts_cloud_free['cloud_free_image_count'])])\n",
    "pd.DataFrame(image_counts_cloud_free[['cloud_free_image_count']]\n",
    "             .groupby('year')\n",
    "             .value_counts()\n",
    "             .sort_index()).rename(columns = {0: 'polygon_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b24dec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.displot(data = image_counts_cloud_free, \n",
    "            x = 'cloud_free_image_count',\n",
    "#             hue = 'year',\n",
    "#             multiple = 'stack',\n",
    "#             alpha = 0.5,\n",
    "            row = 'year'\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bed895",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get count of images by polygon and year\n",
    "image_counts_f1 = images_ordered[['polygon_id', 'year']].value_counts()\n",
    "image_counts_f1 = pd.DataFrame(image_counts_f1, columns = ['count']).sort_index()\n",
    "image_counts_f1 = (\n",
    "    image_counts_f1.join(\n",
    "        images_ordered[['polygon_id', 'year', 'coverage', 'area', 'cloud_cover']]\n",
    "        .groupby(['polygon_id', 'year'])\n",
    "        .aggregate({'coverage': 'sum',\n",
    "                    'area': 'sum',\n",
    "                    'cloud_cover': 'max'})\n",
    "        .rename(columns = {'coverage': 'cumulative_coverage',\n",
    "                           'area': 'cumulative_area',\n",
    "                           'cloud_cover': 'max_cloud_cover'})\n",
    "    ).rename(columns = {'count': 'img_count'})\n",
    ")\n",
    "# image_counts_f1.to_csv('/home/hrodenhizer/Documents/permafrost_pathways/rts_mapping/planet_processing_test/data/automated_download/image_counts_filtered.csv')\n",
    "image_counts_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a41655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 images is the least for any polygon in any year\n",
    "image_counts_f1[image_counts_f1['img_count'] == min(image_counts_f1['img_count'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f3b13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 images is the most for any polygon in any year\n",
    "image_counts_f1[image_counts_f1['img_count'] == max(image_counts_f1['img_count'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a053b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1001% coverage is the least for any polygon in any year\n",
    "image_counts_f1[image_counts_f1['cumulative_coverage'] == min(image_counts_f1['cumulative_coverage'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500d4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1099% coverage is the least for any polygon in any year\n",
    "image_counts_f1[image_counts_f1['cumulative_coverage'] == max(image_counts_f1['cumulative_coverage'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e05832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18% is the max cloud cover in an image\n",
    "image_counts_f1[image_counts_f1['max_cloud_cover'] == max(image_counts_f1['max_cloud_cover'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65670ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total area if all of these images are downloaded\n",
    "total_area = sum(images_ordered['area'])\n",
    "print(total_area)\n",
    "print(total_area/50000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720f9e4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.displot(data = image_counts_f1, \n",
    "            x = 'img_count',\n",
    "#             hue = 'year',\n",
    "#             multiple = 'stack',\n",
    "#             alpha = 0.5,\n",
    "            row = 'year'\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0693e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of RTS polygons by the max cloud cover in the selected images\n",
    "image_counts_f1.groupby('year').max_cloud_cover.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd41c80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.displot(data = image_counts_f1, \n",
    "            x = 'max_cloud_cover',\n",
    "#             hue = 'year',\n",
    "#             multiple = 'stack',\n",
    "#             alpha = 0.5,\n",
    "            row = 'year'\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1ce0b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get info for all orders\n",
    "orders = requests.get('https://api.planet.com/compute/ops/orders/v2/',\n",
    "                      auth=HTTPBasicAuth(api_key, '')).json()\n",
    "all_orders = []\n",
    "all_orders.append(orders['orders'])\n",
    "while len(orders['_links']) >= 2:\n",
    "    orders = requests.get(orders['_links']['next'],\n",
    "                          auth=HTTPBasicAuth(api_key, '')).json()\n",
    "    all_orders.append(orders['orders'])\n",
    "all_orders = [order for page in all_orders for order in page]\n",
    "all_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04969c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2826086",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_items = pd.DataFrame({'polygon_id': [int(item) \n",
    "                                                for items \n",
    "                                                in [[int(order['name'].split('_')[-3])] * len(order['products'][0]['item_ids'])\n",
    "                                                    for order in all_orders] \n",
    "                                                for item in items],\n",
    "                                 'year': [int(item) \n",
    "                                          for items \n",
    "                                          in [[int(order['name'].split('_')[-2])] * len(order['products'][0]['item_ids'])\n",
    "                                              for order in all_orders] \n",
    "                                          for item in items],\n",
    "                                 'id': [item \n",
    "                                        for items \n",
    "                                        in [order['products'][0]['item_ids'] \n",
    "                                            for order in all_orders] \n",
    "                                        for item in items],\n",
    "                                 'order': [int(item) \n",
    "                                           for items \n",
    "                                           in [[order['name']] * len(order['products'][0]['item_ids'])\n",
    "                                                for order in all_orders] \n",
    "                                            for item in items]}).drop_duplicates()\n",
    "ordered_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d48684",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_names = [order['name'] for order in all_orders]\n",
    "order_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15513c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_alignment = (images_ordered\n",
    "                              .join(ordered_items\n",
    "                                    .set_index(['polygon_id', 'year', 'id']), \n",
    "                                    on = ['polygon_id', 'year', 'id'])\n",
    "                              .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9916cb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = orders_alignment.dropna().index\n",
    "orders_alignment.loc[~orders_alignment.index.isin(idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef08fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planet charges a minimum of 100 km^2 per image, even if you you only downloaded a fraction of that!\n",
    "orders_alignment['quota_usage'] = [area if area >= 100 else 100 for area in orders_alignment.area ]\n",
    "total_area = sum(orders_alignment[orders_alignment.index.isin(idx)].quota_usage)\n",
    "total_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce1cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### for some reason planet thinks I have downloaded 447,487 km^2\n",
    "dc = 25*11.5 # average area of entire PS2 image\n",
    "dr = 25*23.0 # average area of entire PS2.SD image\n",
    "sd = 32.5*19.6 # average area of entire PSB.SD image\n",
    "print(round(total_area), # area calculated previously - has been confirmed that downloaded image size was equal to this calculation\n",
    "      len(ordered_items)*dc, # area if quota usage was calculated on entire image area (not clipped area) for all images coming from PS2\n",
    "      len(ordered_items)*dr, # area if quota usage was calculated on entire image area (not clipped area) for all images coming from PS2.SD\n",
    "      len(ordered_items)*sd) # area if quota usage was calculated on entire image area (not clipped area) for all images coming from PSB.SD\n",
    "# none of those values are close to what planet says I have downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319537a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
